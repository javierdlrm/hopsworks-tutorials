{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab737cc9",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 04: LLM Inference</span>\n",
    "    \n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Connect to the Hopsworks AI Lakehouse\n",
    "2. Retrieve the feature view and predictor.\n",
    "3. Load the LLM.\n",
    "4. Configure langchain and the context manager\n",
    "5. Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1987f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8549897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from functions.llm_chain import (\n",
    "    load_model, \n",
    "    get_llm_chain, \n",
    "    generate_response,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61b6a6",
   "metadata": {},
   "source": [
    "## Connect to Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b24be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-19 13:54:24,439 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://hopsworks0.logicalclocks.com/p/119\n"
     ]
    }
   ],
   "source": [
    "# connect to Hopsworks\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9f450",
   "metadata": {},
   "source": [
    "## Get Departures Feature View and Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16914c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 'air_quality_fv' feature view\n",
    "feature_view = fs.get_feature_view(\n",
    "    name='departures_agg',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "# Initialize batch scoring\n",
    "feature_view.init_batch_scoring(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03dbe295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model serving\n",
    "ms = project.get_model_serving()\n",
    "\n",
    "# Retrieve bitcoin predictor\n",
    "model_deployment = ms.get_deployment(\"latedeparturemodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfe8d6",
   "metadata": {},
   "source": [
    "## ‚¨áÔ∏è LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19158a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-19 13:59:05,962 WARNING: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-19 13:59:09,864 INFO: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f8e9da46844fb28212edc6950fa962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-19 13:59:21,416 WARNING: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the LLM and its corresponding tokenizer.\n",
    "model_llm, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc26e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 13:59:22 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100S-PCIE-32GB          On  |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   44C    P0             41W /  250W |   28964MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cc5d8",
   "metadata": {},
   "source": [
    "## ‚õìÔ∏è LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f5c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure a language model chain.\n",
    "llm_chain = get_llm_chain(\n",
    "    tokenizer,\n",
    "    model_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640a4f5",
   "metadata": {},
   "source": [
    "## üß¨ Model Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a784212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóìÔ∏è Today's date: Thursday, 2024-09-19\n",
      "üìñ \n",
      "===============\n",
      "\n",
      "Hello! I am a route planner assistant that can help you with analyzing historical public transport departures in Stockholm. I can provide information about the frequency and number of late departures to help you plan your journey better.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"Hi! who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3900b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.07s) \n",
      "Scheduled time: 2024-09-19\n",
      "Departure key: 9295-2024-09-19T04:34:57+02:00\n",
      "üóìÔ∏è Today's date: Thursday, 2024-09-19\n",
      "üìñ {'lateness_probability': 0.00020051002502441406, 'scheduled_time': '2024-09-19'}\n",
      "===============\n",
      "\n",
      "Yes, there is a possibility of late departures today. However, the probability is quite low at 0.02005%. It is still recommended to leave a little earlier to account for any unforeseen delays.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"Are there expected late departures today?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6901d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Were there expected late departures yesterday?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8baacb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.69s) \n",
      "üóìÔ∏è Today's date: Thursday, 2024-09-19\n",
      "üìñ Departures information between 2024-09-10 and 2024-09-19:\n",
      "Date: 2024-09-18 02:31:00+00:00; Expected: 2024-09-18 02:31:00+00:00; Number of issues: 0.0; Number of late departures: 0.0; Number of deviations: 0.0; Deviations severity: 0.0;\n",
      "Date: 2024-09-18 02:53:30+00:00; Expected: 2024-09-18 02:53:43+00:00; Number of issues: 0.0; Number of late departures: 0.0; Number of deviations: 0.0; Deviations severity: 0.0;\n",
      "Date: 2024-09-18 09:04:00+00:00; Expected: 2024-09-18 09:05:16+00:00; Number of issues: 0.0; Number of late departures: 0.0; Number of deviations: 0.0; Deviations severity: 0.0;\n",
      "Date: 2024-09-18 09:06:00+00:00; Expected: 2024-09-18 09:09:52+00:00; Number of issues: 0.0; Number of late departures: 0.0; Number of deviations: 0.0; Deviations severity: 0.0;\n",
      "Date: 2024-09-18 21:53:00+00:00; Expected: 2024-09-18 21:54:21+00:00; Number of issues: 0.0; Number of late departures: 0.0; Number of deviations: 0.0; Deviations severity: 0.0;\n",
      "Date: 2024-09-18 22:18:00+00:00; Expected: 2024-09-18 22:18:37+00:00; Number of issues: 0.0; Number of late departures: 0.0; Number of deviations: 0.0; Deviations severity: 0.0;\n",
      "===============\n",
      "\n",
      "Between 2024-09-10 and 2024-09-19, there have been no late departures reported.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"How many late departures are from 2024-09-10 till 2024-09-19?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d19624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.58s) \n",
      "üóìÔ∏è Today's date: Thursday, 2024-09-19\n",
      "üìñ Not information found about Departures on this date range\n",
      "===============\n",
      "\n",
      "Last month, there were no reported departure issues on the public transport in Stockholm.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"How many departure issues occurred last month?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view, \n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee129560",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"How many departure deviations were planned for last month?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION, \n",
    "    feature_view, \n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e8683",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "QUESTION = \"How long will take me to commute if I leave now?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0b282",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "QUESTION = \"At what time should I leave if I want to reach before 10:00 to my office?\"\n",
    "\n",
    "response = generate_response(\n",
    "    QUESTION,\n",
    "    feature_view,\n",
    "    model_deployment,\n",
    "    model_llm,\n",
    "    tokenizer,\n",
    "    llm_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
