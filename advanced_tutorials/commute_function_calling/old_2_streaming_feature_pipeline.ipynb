{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140660dc",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Streaming Feature Pipeline</span>\n",
    "\n",
    "## 🗒️ This notebook is divided into the following sections:\n",
    "1. Connect to the Hopsworks AI Lakehouse.\n",
    "2. Retrieve Feature Groups for departures.\n",
    "3. Consume departures events from Kafka topic \"departures.\n",
    "4. Process the events and compute windowed aggregations\n",
    "5. Write the resulting aggregated features to the correspoding Feature Groups.\n",
    "\n",
    "**NOTE:** Before going through this notebook, the following python scripts need to be executed:\n",
    "- *setup/feature_group.py*: creates the Feature Groups for departures, and aggregated information of departures.\n",
    "- *reply_site_departures.py*: fetch real-time departures information from SL's Stockholm Public Transportation services and reply the events to the Kafka topic \"departures\".\n",
    "\n",
    "\n",
    "![tutorial-flow](images/01_featuregroups.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ebea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Hopsworks\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "kafka_api = project.get_kafka_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65934cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get departures feature groups\n",
    "departures_fg = fs.get_feature_group(\"departures\", 2)\n",
    "departures_agg_30m_fg = fs.get_feature_group(\"departures_agg_30m\", 1)\n",
    "departures_agg_1h_fg = fs.get_feature_group(\"departures_agg_1h\", 1)\n",
    "departures_agg_6h_fg = fs.get_feature_group(\"departures_agg_6h\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ec144",
   "metadata": {},
   "source": [
    "### Streaming feature pipeline\n",
    "\n",
    "We use **QuixStreams**, a streaming processing engine, to consume the departures events and compute the aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56323df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default Kafka configuration\n",
    "\n",
    "def get_consumer_config():\n",
    "    kafka_config = kafka_api.get_default_config()\n",
    "    consumer_config = kafka_config\n",
    "    consumer_config['default.topic.config'] = {'auto.offset.reset': 'latest'}\n",
    "    consumer_config['partition.assignment.strategy'] = \"cooperative-sticky\"\n",
    "    return consumer_config\n",
    "\n",
    "def get_producer_config():\n",
    "    from hsfs.core import kafka_engine\n",
    "    producer_config = kafka_engine.get_kafka_config(\n",
    "        departures_fg.feature_store_id, {}\n",
    "    )\n",
    "    return producer_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e940d",
   "metadata": {},
   "source": [
    "##### Create QuixStreams Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dce0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quixstreams import Application, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "\n",
    "def on_consumer_error(*args, **kwargs):\n",
    "    print(\"ON CONSUMER ERROR\")\n",
    "    if args is not None:\n",
    "        print(args)\n",
    "    if kwargs is not None:\n",
    "        print(kwargs)\n",
    "    \n",
    "def on_processing_error(*args, **kwargs):\n",
    "    print(\"ON PROCESSING ERROR\")\n",
    "    if args is not None:\n",
    "        print(args)\n",
    "    if kwargs is not None:\n",
    "        print(kwargs)\n",
    "    \n",
    "def on_producer_error(*args, **kwargs):\n",
    "    print(\"ON PRODUCER ERROR\")\n",
    "    if args is not None:\n",
    "        print(args)\n",
    "    if kwargs is not None:\n",
    "        print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QuixStreams Application\n",
    "\n",
    "app = Application(\n",
    "    broker_address=kafka_config[\"bootstrap.servers\"],\n",
    "    auto_create_topics=False,\n",
    "    #loglevel = \"DEBUG\",\n",
    "    \n",
    "    # consumer\n",
    "    consumer_extra_config=consumer_config,\n",
    "    consumer_group=\"my-group-id\",\n",
    "    on_consumer_error=on_consumer_error,\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    use_changelog_topics=False,\n",
    "    \n",
    "    # producer\n",
    "    producer_extra_config=producer_config,\n",
    "    on_producer_error=on_producer_error,\n",
    "    \n",
    "    # processing\n",
    "    on_processing_error=on_processing_error,\n",
    ")\n",
    "\n",
    "try:\n",
    "    app.clear_state()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consumer topic\n",
    "\n",
    "input_topic = app.topic(name=\"departures\", value_serializer=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1f753",
   "metadata": {},
   "source": [
    "**Processing functions and multi_part_insert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_part_insert(event: dict, feature_group):\n",
    "    event_df = to_pandas(event)\n",
    "    with feature_group.multi_part_insert() as writer:\n",
    "        writer.insert(event_df)\n",
    "    print(\">> Event delived successfully to \" + feature_group._online_topic_name + \"_\" + feature_group.name)\n",
    "    \n",
    "def to_pandas(event: dict):\n",
    "    df = pd.DataFrame(event, index=[0])\n",
    "    \n",
    "    # parse to float32\n",
    "    cols = list(df.columns)\n",
    "    for col_name in [\"departure_id\", \"departure_agg_id\", \"site_id\", \"scheduled\", \"first_scheduled\", \"expected\", \"state\", \"journey_state\", \"journey_prediction_state\", \"late\"]:\n",
    "        if col_name in cols:\n",
    "            cols.remove(col_name)\n",
    "    df[cols] = df[cols].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bbfb1d",
   "metadata": {},
   "source": [
    "**Reduce functions for windowed aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bceed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer and Reducer for first aggregation\n",
    "\n",
    "def num_state_issues(state):\n",
    "    return 1 if state in [\"CANCELLED\", \"INHIBITED\", \"MISSED\", \"REPLACED\"] else 0\n",
    "\n",
    "def num_journey_state_issues(journey_state):\n",
    "    return 1 if journey_state in [\"SLOWPROGRESS\", \"NOPROGRESS\", \"OFFROUTE\", \"ABORTED\", \"CANCELLED\"] else 0\n",
    "\n",
    "def num_journey_prediction_state_issues(journey_prediction_state):\n",
    "    return 1 if journey_prediction_state in [\"LOSTCONTACT\", \"UNRELIABLE\"] else 0\n",
    "\n",
    "def get_earlier_datetime_str(t1, t2):\n",
    "    min_t = min(datetime.fromisoformat(t1), datetime.fromisoformat(t2))\n",
    "    return min_t.isoformat()\n",
    "\n",
    "def initializer_agg(event: dict) -> dict:\n",
    "    value = {\n",
    "        \"departure_agg_id\": event[\"site_id\"] + \"-\" + event[\"scheduled\"],\n",
    "        \"site_id\": event[\"site_id\"],\n",
    "        \"first_scheduled\": event[\"scheduled\"],\n",
    "        \"state_issue_count\": num_state_issues(event[\"state\"]),\n",
    "        \"journey_state_issue_count\": num_journey_state_issues(event[\"journey_state\"]),\n",
    "        \"journey_prediction_state_issue_count\": num_journey_prediction_state_issues(event[\"journey_prediction_state\"]),\n",
    "        \"deviations_count_min\": event[\"deviations_count\"],\n",
    "        \"deviations_count_max\": event[\"deviations_count\"],\n",
    "        \"deviations_count_sum\": event[\"deviations_count\"],\n",
    "        \"deviations_count_count\": 1,\n",
    "        \"deviations_count_mean\": event[\"deviations_count\"],\n",
    "        \"deviations_importance_min\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_max\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_sum\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_count\": 1,\n",
    "        \"deviations_importance_mean\": event[\"deviations_importance_max\"],\n",
    "        \"late_count\": 1 if event[\"late\"] else 0,\n",
    "    }\n",
    "    return json.loads(json.dumps(value))\n",
    "\n",
    "def reducer_agg(aggregated: dict, event: dict) -> dict:\n",
    "    return {\n",
    "        \"departure_agg_id\": aggregated[\"site_id\"] + \"-\" + aggregated[\"first_scheduled\"],\n",
    "        \"site_id\": aggregated[\"site_id\"],\n",
    "        \"first_scheduled\": get_earlier_datetime_str(aggregated[\"first_scheduled\"], event[\"scheduled\"]),\n",
    "        \"state_issue_count\": aggregated[\"state_issue_count\"] + num_state_issues(event[\"state\"]),\n",
    "        \"journey_state_issue_count\": aggregated[\"journey_state_issue_count\"] + num_journey_state_issues(event[\"journey_state\"]),\n",
    "        \"journey_prediction_state_issue_count\": aggregated[\"journey_prediction_state_issue_count\"] + num_journey_prediction_state_issues(event[\"journey_prediction_state\"]),\n",
    "        \"deviations_count_min\": min(aggregated[\"deviations_count_min\"], event[\"deviations_count\"]),\n",
    "        \"deviations_count_max\": max(aggregated[\"deviations_count_max\"], event[\"deviations_count\"]),\n",
    "        \"deviations_count_sum\": aggregated[\"deviations_count_sum\"] + event[\"deviations_count\"],\n",
    "        \"deviations_count_count\": aggregated[\"deviations_count_count\"] + 1,\n",
    "        \"deviations_count_mean\": float(aggregated[\"deviations_count_sum\"] + event[\"deviations_count\"]) / (aggregated[\"deviations_count_count\"] + 1),\n",
    "        \"deviations_importance_min\": min(aggregated[\"deviations_importance_min\"], event[\"deviations_importance_max\"]),\n",
    "        \"deviations_importance_max\": max(aggregated[\"deviations_importance_max\"], event[\"deviations_importance_max\"]),\n",
    "        \"deviations_importance_sum\": aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_count\": aggregated[\"deviations_importance_count\"] + 1,\n",
    "        \"deviations_importance_mean\": float(aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_max\"]) / (aggregated[\"deviations_importance_count\"] + 1),\n",
    "        \"late_count\": 1 if event[\"late\"] else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Initializer and Reducer for Accumulative Aggregations\n",
    "\n",
    "def initializer_acc_agg(event: dict) -> dict:\n",
    "    return {\n",
    "        \"departure_agg_id\": event[\"site_id\"] + \"-\" + event[\"first_scheduled\"],\n",
    "        \"site_id\": event[\"site_id\"],\n",
    "        \"first_scheduled\": event[\"first_scheduled\"],\n",
    "        \"state_issue_count\": event[\"state_issue_count\"],\n",
    "        \"journey_state_issue_count\": event[\"journey_state_issue_count\"],\n",
    "        \"journey_prediction_state_issue_count\": event[\"journey_prediction_state_issue_count\"],\n",
    "        \"deviations_count_min\": event[\"deviations_count_min\"],\n",
    "        \"deviations_count_max\": event[\"deviations_count_max\"],\n",
    "        \"deviations_count_sum\": event[\"deviations_count_sum\"],\n",
    "        \"deviations_count_count\": 1,\n",
    "        \"deviations_count_mean\": event[\"deviations_count_mean\"],\n",
    "        \"deviations_importance_min\": event[\"deviations_importance_min\"],\n",
    "        \"deviations_importance_max\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_sum\": event[\"deviations_importance_sum\"],\n",
    "        \"deviations_importance_count\": 1,\n",
    "        \"deviations_importance_mean\": event[\"deviations_importance_mean\"],\n",
    "        \"late_count\": event[\"late_count\"],\n",
    "    }\n",
    "\n",
    "def reducer_acc_agg(aggregated: dict, event: dict) -> dict:\n",
    "    return {\n",
    "        \"departure_agg_id\": aggregated[\"site_id\"] + \"-\" + aggregated[\"first_scheduled\"],\n",
    "        \"site_id\": event[\"site_id\"],\n",
    "        \"first_scheduled\": get_earlier_datetime_str(aggregated[\"first_scheduled\"], event[\"first_scheduled\"]),\n",
    "        \"state_issue_count\": aggregated[\"state_issue_count\"] + event[\"state_issue_count\"],\n",
    "        \"journey_state_issue_count\": aggregated[\"journey_state_issue_count\"] + event[\"journey_state_issue_count\"],\n",
    "        \"journey_prediction_state_issue_count\": aggregated[\"journey_prediction_state_issue_count\"] + event[\"journey_prediction_state_issue_count\"],\n",
    "        \"deviations_count_min\": min(aggregated[\"deviations_count_min\"], event[\"deviations_count_min\"]),\n",
    "        \"deviations_count_max\": max(aggregated[\"deviations_count_max\"], event[\"deviations_count_max\"]),\n",
    "        \"deviations_count_sum\": aggregated[\"deviations_count_sum\"] + event[\"deviations_count_sum\"],\n",
    "        \"deviations_count_count\": aggregated[\"deviations_count_count\"] + 1,\n",
    "        \"deviations_count_mean\": float(aggregated[\"deviations_count_sum\"] + event[\"deviations_count_mean\"]) / (aggregated[\"deviations_count_count\"] + 1),\n",
    "        \"deviations_importance_min\": min(aggregated[\"deviations_importance_min\"], event[\"deviations_importance_min\"]),\n",
    "        \"deviations_importance_max\": max(aggregated[\"deviations_importance_max\"], event[\"deviations_importance_max\"]),\n",
    "        \"deviations_importance_sum\": aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_sum\"],\n",
    "        \"deviations_importance_count\": aggregated[\"deviations_importance_count\"] + 1,\n",
    "        \"deviations_importance_mean\": float(aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_mean\"]) / (aggregated[\"deviations_importance_count\"] + 1),\n",
    "        \"late_count\": aggregated[\"late_count\"] + event[\"late_count\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be51a1",
   "metadata": {},
   "source": [
    "**Create a Streaming DataFrame and define the aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streaming DataFrame\n",
    "\n",
    "sdf = app.dataframe(input_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5bccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast data types\n",
    "sdf[\"scheduled\"] = sdf[\"scheduled\"].apply(datetime.fromisoformat)\n",
    "sdf[\"expected\"] = sdf[\"expected\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert all events to departures feature group\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_fg))\n",
    "\n",
    "# drop expected field since it's not needed in the agg fgs\n",
    "sdf.drop(\"expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  1st Aggregation - 30 Minutes\n",
    "#\n",
    "\n",
    "# convert datetime to str - for serialization of the state\n",
    "sdf[\"scheduled\"] = sdf[\"scheduled\"].apply(lambda t: t.isoformat())\n",
    "\n",
    "sdf.apply(lambda value: print('Event: ', value))\n",
    "\n",
    "# perform window 1-minute aggregations\n",
    "sdf = (\n",
    "    # Define a tumbling window\n",
    "    sdf.tumbling_window(timedelta(seconds=15))  # set 5 seconds for demo and debugging\n",
    "\n",
    "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
    "    .reduce(reducer=reducer_agg, initializer=initializer_agg)\n",
    "\n",
    "    # Emit results only for closed windows\n",
    "    .final()\n",
    "    \n",
    "    # extract value\n",
    "    .apply(lambda result: result[\"value\"])\n",
    ")\n",
    "\n",
    "# revert timestamp string to datetime\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert to departures_agg feature group\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_agg_30m_fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  2nd Aggregation - 2 Minutes\n",
    "#\n",
    "\n",
    "# convert datetime to str - for serialization of the state\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(lambda t: t.isoformat())\n",
    "\n",
    "# perform window aggregations for 2 minutes\n",
    "sdf = (\n",
    "    # Define a tumbling window\n",
    "    sdf.tumbling_window(timedelta(seconds=30)) # set 10 seconds for demo and debugging\n",
    "\n",
    "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
    "    .reduce(reducer=reducer_acc_agg, initializer=initializer_acc_agg)\n",
    "\n",
    "    # Emit results only for closed windows\n",
    "    .final()\n",
    "    \n",
    "    # extract value\n",
    "    .apply(lambda result: result[\"value\"])\n",
    ")\n",
    "\n",
    "# revert timestamp string to datetime\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert to departures_agg FG topic\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_agg_1h_fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  3rd Aggregation - 3 Minutes\n",
    "#\n",
    "\n",
    "# convert datetime to str - for serialization of the state\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(lambda t: t.isoformat())\n",
    "\n",
    "# perform window aggregations for 2 minutes\n",
    "sdf = (\n",
    "    # Define a tumbling window\n",
    "    sdf.tumbling_window(timedelta(seconds=60))  # set 15 seconds for demo and debugging\n",
    "\n",
    "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
    "    .reduce(reducer=reducer_acc_agg, initializer=initializer_acc_agg)\n",
    "\n",
    "    # Emit results only for closed windows\n",
    "    .final()\n",
    "    \n",
    "    # extract value\n",
    "    .apply(lambda result: result[\"value\"])\n",
    ")\n",
    "\n",
    "# revert timestamp string to datetime\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert to departures_agg FG topic\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_agg_6h_fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8eeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start streaming feature pipeline\n",
    "\n",
    "app.run(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66387adf",
   "metadata": {},
   "source": [
    "#### Materialize departures and departures aggregated features into the Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c7d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: departures_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks0.logicalclocks.com/p/119/jobs/named/departures_1_offline_fg_materialization/executions\n",
      "2024-09-18 09:16:31,346 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2024-09-18 09:16:34,393 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2024-09-18 09:17:41,603 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2024-09-18 09:17:41,632 INFO: Waiting for log aggregation to finish.\n",
      "2024-09-18 09:17:49,846 INFO: Execution finished successfully.\n",
      "Launching job: departures_agg_30m_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks0.logicalclocks.com/p/119/jobs/named/departures_agg_30m_1_offline_fg_materialization/executions\n",
      "2024-09-18 09:17:58,276 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2024-09-18 09:18:01,326 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2024-09-18 09:19:08,372 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2024-09-18 09:19:08,401 INFO: Waiting for log aggregation to finish.\n",
      "2024-09-18 09:19:16,623 INFO: Execution finished successfully.\n",
      "Launching job: departures_agg_1h_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks0.logicalclocks.com/p/119/jobs/named/departures_agg_1h_1_offline_fg_materialization/executions\n",
      "2024-09-18 09:19:25,039 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2024-09-18 09:19:28,090 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2024-09-18 09:20:32,138 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2024-09-18 09:20:32,169 INFO: Waiting for log aggregation to finish.\n",
      "2024-09-18 09:20:40,399 INFO: Execution finished successfully.\n",
      "Launching job: departures_agg_6h_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://hopsworks0.logicalclocks.com/p/119/jobs/named/departures_agg_6h_1_offline_fg_materialization/executions\n",
      "2024-09-18 09:20:48,814 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2024-09-18 09:20:51,865 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2024-09-18 09:21:59,002 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2024-09-18 09:21:59,030 INFO: Waiting for log aggregation to finish.\n",
      "2024-09-18 09:22:07,254 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Execution('SUCCEEDED', 'FINISHED', '2024-09-18T09:20:40.000Z', '-op offline_fg_materialization -path hdfs:///Projects/commute/Resources/jobs/departures_agg_6h_1_offline_fg_materialization/config_1726618479692')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start materialization job\n",
    "departures_fg.materialization_job.run(await_termination=True)\n",
    "departures_agg_30m_fg.materialization_job.run(await_termination=True)\n",
    "departures_agg_1h_fg.materialization_job.run(await_termination=True)\n",
    "departures_agg_6h_fg.materialization_job.run(await_termination=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ccd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# departures_agg_30m_fg.materialization_job.run(await_termination=False)\n",
    "# departures_agg_1h_fg.materialization_job.run(await_termination=False)\n",
    "# departures_agg_6h_fg.materialization_job.run(await_termination=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
