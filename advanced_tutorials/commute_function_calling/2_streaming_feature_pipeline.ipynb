{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9c02c0",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Streaming Feature Pipeline</span>\n",
    "\n",
    "## ðŸ—’ï¸ This notebook is divided into the following sections:\n",
    "1. Connect to the Hopsworks AI Lakehouse.\n",
    "2. Retrieve Feature Groups for departures.\n",
    "3. Consume departures events from Kafka topic \"departures.\n",
    "4. Process the events and compute windowed aggregations\n",
    "5. Write the resulting aggregated features to the correspoding Feature Groups.\n",
    "\n",
    "**NOTE:** Before going through this notebook, the following python scripts need to be executed:\n",
    "- *setup/feature_group.py*: creates the Feature Groups for departures, and aggregated information of departures.\n",
    "- *reply_site_departures.py*: fetch real-time departures information from SL's Stockholm Public Transportation services and reply the events to the Kafka topic \"departures\".\n",
    "\n",
    "\n",
    "![tutorial-flow](images/01_featuregroups.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1628dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-20 08:20:56,097 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://hopsworks0.logicalclocks.com/p/119\n"
     ]
    }
   ],
   "source": [
    "# connect to Hopsworks\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()\n",
    "kafka_api = project.get_kafka_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77359518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other imports\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992d4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get departures feature groups\n",
    "departures_fg = fs.get_feature_group(\"departures\", 2)\n",
    "departures_agg_30m_fg = fs.get_feature_group(\"departures_agg_30m\", 1)\n",
    "departures_agg_1h_fg = fs.get_feature_group(\"departures_agg_1h\", 1)\n",
    "departures_agg_6h_fg = fs.get_feature_group(\"departures_agg_6h\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5788084a",
   "metadata": {},
   "source": [
    "### Streaming feature pipeline\n",
    "\n",
    "We use **QuixStreams**, a streaming processing engine, to consume the departures events and compute the aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d86f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default Kafka configuration\n",
    "\n",
    "kafka_config = kafka_api.get_default_config()\n",
    "\n",
    "def get_consumer_config():\n",
    "    consumer_config = kafka_config\n",
    "    consumer_config['default.topic.config'] = {'auto.offset.reset': 'latest'}\n",
    "    consumer_config['partition.assignment.strategy'] = \"cooperative-sticky\"\n",
    "    return consumer_config\n",
    "\n",
    "def get_producer_config():\n",
    "    from hsfs.core import kafka_engine\n",
    "    producer_config = kafka_engine.get_kafka_config(\n",
    "        departures_fg.feature_store_id, {}\n",
    "    )\n",
    "    return producer_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d348086",
   "metadata": {},
   "source": [
    "##### Create QuixStreams Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b53414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-20 08:20:57,516 WARNING: DeprecationWarning: jsonschema.RefResolver is deprecated as of v4.18.0, in favor of the https://github.com/python-jsonschema/referencing library, which provides more compliant referencing behavior as well as more flexible APIs for customization. A future release will remove RefResolver. Please file a feature request (on referencing) if you are missing an API for the kind of customization you need.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from quixstreams import Application, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "672cd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "\n",
    "def on_consumer_error(*args, **kwargs):\n",
    "    print(\"ON CONSUMER ERROR\")\n",
    "    if args is not None:\n",
    "        print(args)\n",
    "    if kwargs is not None:\n",
    "        print(kwargs)\n",
    "    \n",
    "def on_processing_error(*args, **kwargs):\n",
    "    print(\"ON PROCESSING ERROR\")\n",
    "    if args is not None:\n",
    "        print(args)\n",
    "    if kwargs is not None:\n",
    "        print(kwargs)\n",
    "    \n",
    "def on_producer_error(*args, **kwargs):\n",
    "    print(\"ON PRODUCER ERROR\")\n",
    "    if args is not None:\n",
    "        print(args)\n",
    "    if kwargs is not None:\n",
    "        print(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfaadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QuixStreams Application\n",
    "\n",
    "app = Application(\n",
    "    broker_address=kafka_config[\"bootstrap.servers\"],\n",
    "    auto_create_topics=False,\n",
    "    #loglevel = \"DEBUG\",\n",
    "    \n",
    "    # consumer\n",
    "    consumer_extra_config=get_consumer_config(),\n",
    "    consumer_group=\"my-group-id\",\n",
    "    on_consumer_error=on_consumer_error,\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    use_changelog_topics=False,\n",
    "    \n",
    "    # producer\n",
    "    producer_extra_config=get_producer_config(),\n",
    "    on_producer_error=on_producer_error,\n",
    "    \n",
    "    # processing\n",
    "    on_processing_error=on_processing_error,\n",
    ")\n",
    "\n",
    "try:\n",
    "    app.clear_state()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8550c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consumer topic\n",
    "\n",
    "input_topic = app.topic(name=\"departures\", value_serializer=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d9e85",
   "metadata": {},
   "source": [
    "**Processing functions and multi_part_insert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cf1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_part_insert(event: dict, feature_group):\n",
    "    event_df = to_pandas(event)\n",
    "    with feature_group.multi_part_insert() as writer:\n",
    "        writer.insert(event_df)\n",
    "    print(\">> Event delived successfully to \" + feature_group._online_topic_name + \"_\" + feature_group.name)\n",
    "    \n",
    "def to_pandas(event: dict):\n",
    "    df = pd.DataFrame(event, index=[0])\n",
    "    \n",
    "    # parse to float32\n",
    "    cols = list(df.columns)\n",
    "    for col_name in [\"departure_id\", \"departure_agg_id\", \"site_id\", \"scheduled\", \"first_scheduled\", \"expected\", \"state\", \"journey_state\", \"journey_prediction_state\", \"late\"]:\n",
    "        if col_name in cols:\n",
    "            cols.remove(col_name)\n",
    "    df[cols] = df[cols].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453ce00",
   "metadata": {},
   "source": [
    "**Reduce functions for windowed aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f13e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer and Reducer for first aggregation\n",
    "\n",
    "def num_state_issues(state):\n",
    "    return 1 if state in [\"CANCELLED\", \"INHIBITED\", \"MISSED\", \"REPLACED\"] else 0\n",
    "\n",
    "def num_journey_state_issues(journey_state):\n",
    "    return 1 if journey_state in [\"SLOWPROGRESS\", \"NOPROGRESS\", \"OFFROUTE\", \"ABORTED\", \"CANCELLED\"] else 0\n",
    "\n",
    "def num_journey_prediction_state_issues(journey_prediction_state):\n",
    "    return 1 if journey_prediction_state in [\"LOSTCONTACT\", \"UNRELIABLE\"] else 0\n",
    "\n",
    "def get_earlier_datetime_str(t1, t2):\n",
    "    min_t = min(datetime.fromisoformat(t1), datetime.fromisoformat(t2))\n",
    "    return min_t.isoformat()\n",
    "\n",
    "def initializer_agg(event: dict) -> dict:\n",
    "    value = {\n",
    "        \"departure_agg_id\": event[\"site_id\"] + \"-\" + event[\"scheduled\"],\n",
    "        \"site_id\": event[\"site_id\"],\n",
    "        \"first_scheduled\": event[\"scheduled\"],\n",
    "        \"state_issue_count\": num_state_issues(event[\"state\"]),\n",
    "        \"journey_state_issue_count\": num_journey_state_issues(event[\"journey_state\"]),\n",
    "        \"journey_prediction_state_issue_count\": num_journey_prediction_state_issues(event[\"journey_prediction_state\"]),\n",
    "        \"deviations_count_min\": event[\"deviations_count\"],\n",
    "        \"deviations_count_max\": event[\"deviations_count\"],\n",
    "        \"deviations_count_sum\": event[\"deviations_count\"],\n",
    "        \"deviations_count_count\": 1,\n",
    "        \"deviations_count_mean\": event[\"deviations_count\"],\n",
    "        \"deviations_importance_min\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_max\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_sum\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_count\": 1,\n",
    "        \"deviations_importance_mean\": event[\"deviations_importance_max\"],\n",
    "        \"late_count\": 1 if event[\"late\"] else 0,\n",
    "    }\n",
    "    return json.loads(json.dumps(value))\n",
    "\n",
    "def reducer_agg(aggregated: dict, event: dict) -> dict:\n",
    "    return {\n",
    "        \"departure_agg_id\": aggregated[\"site_id\"] + \"-\" + aggregated[\"first_scheduled\"],\n",
    "        \"site_id\": aggregated[\"site_id\"],\n",
    "        \"first_scheduled\": get_earlier_datetime_str(aggregated[\"first_scheduled\"], event[\"scheduled\"]),\n",
    "        \"state_issue_count\": aggregated[\"state_issue_count\"] + num_state_issues(event[\"state\"]),\n",
    "        \"journey_state_issue_count\": aggregated[\"journey_state_issue_count\"] + num_journey_state_issues(event[\"journey_state\"]),\n",
    "        \"journey_prediction_state_issue_count\": aggregated[\"journey_prediction_state_issue_count\"] + num_journey_prediction_state_issues(event[\"journey_prediction_state\"]),\n",
    "        \"deviations_count_min\": min(aggregated[\"deviations_count_min\"], event[\"deviations_count\"]),\n",
    "        \"deviations_count_max\": max(aggregated[\"deviations_count_max\"], event[\"deviations_count\"]),\n",
    "        \"deviations_count_sum\": aggregated[\"deviations_count_sum\"] + event[\"deviations_count\"],\n",
    "        \"deviations_count_count\": aggregated[\"deviations_count_count\"] + 1,\n",
    "        \"deviations_count_mean\": float(aggregated[\"deviations_count_sum\"] + event[\"deviations_count\"]) / (aggregated[\"deviations_count_count\"] + 1),\n",
    "        \"deviations_importance_min\": min(aggregated[\"deviations_importance_min\"], event[\"deviations_importance_max\"]),\n",
    "        \"deviations_importance_max\": max(aggregated[\"deviations_importance_max\"], event[\"deviations_importance_max\"]),\n",
    "        \"deviations_importance_sum\": aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_count\": aggregated[\"deviations_importance_count\"] + 1,\n",
    "        \"deviations_importance_mean\": float(aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_max\"]) / (aggregated[\"deviations_importance_count\"] + 1),\n",
    "        \"late_count\": 1 if event[\"late\"] else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Initializer and Reducer for Accumulative Aggregations\n",
    "\n",
    "def initializer_acc_agg(event: dict) -> dict:\n",
    "    return {\n",
    "        \"departure_agg_id\": event[\"site_id\"] + \"-\" + event[\"first_scheduled\"],\n",
    "        \"site_id\": event[\"site_id\"],\n",
    "        \"first_scheduled\": event[\"first_scheduled\"],\n",
    "        \"state_issue_count\": event[\"state_issue_count\"],\n",
    "        \"journey_state_issue_count\": event[\"journey_state_issue_count\"],\n",
    "        \"journey_prediction_state_issue_count\": event[\"journey_prediction_state_issue_count\"],\n",
    "        \"deviations_count_min\": event[\"deviations_count_min\"],\n",
    "        \"deviations_count_max\": event[\"deviations_count_max\"],\n",
    "        \"deviations_count_sum\": event[\"deviations_count_sum\"],\n",
    "        \"deviations_count_count\": 1,\n",
    "        \"deviations_count_mean\": event[\"deviations_count_mean\"],\n",
    "        \"deviations_importance_min\": event[\"deviations_importance_min\"],\n",
    "        \"deviations_importance_max\": event[\"deviations_importance_max\"],\n",
    "        \"deviations_importance_sum\": event[\"deviations_importance_sum\"],\n",
    "        \"deviations_importance_count\": 1,\n",
    "        \"deviations_importance_mean\": event[\"deviations_importance_mean\"],\n",
    "        \"late_count\": event[\"late_count\"],\n",
    "    }\n",
    "\n",
    "def reducer_acc_agg(aggregated: dict, event: dict) -> dict:\n",
    "    return {\n",
    "        \"departure_agg_id\": aggregated[\"site_id\"] + \"-\" + aggregated[\"first_scheduled\"],\n",
    "        \"site_id\": event[\"site_id\"],\n",
    "        \"first_scheduled\": get_earlier_datetime_str(aggregated[\"first_scheduled\"], event[\"first_scheduled\"]),\n",
    "        \"state_issue_count\": aggregated[\"state_issue_count\"] + event[\"state_issue_count\"],\n",
    "        \"journey_state_issue_count\": aggregated[\"journey_state_issue_count\"] + event[\"journey_state_issue_count\"],\n",
    "        \"journey_prediction_state_issue_count\": aggregated[\"journey_prediction_state_issue_count\"] + event[\"journey_prediction_state_issue_count\"],\n",
    "        \"deviations_count_min\": min(aggregated[\"deviations_count_min\"], event[\"deviations_count_min\"]),\n",
    "        \"deviations_count_max\": max(aggregated[\"deviations_count_max\"], event[\"deviations_count_max\"]),\n",
    "        \"deviations_count_sum\": aggregated[\"deviations_count_sum\"] + event[\"deviations_count_sum\"],\n",
    "        \"deviations_count_count\": aggregated[\"deviations_count_count\"] + 1,\n",
    "        \"deviations_count_mean\": float(aggregated[\"deviations_count_sum\"] + event[\"deviations_count_mean\"]) / (aggregated[\"deviations_count_count\"] + 1),\n",
    "        \"deviations_importance_min\": min(aggregated[\"deviations_importance_min\"], event[\"deviations_importance_min\"]),\n",
    "        \"deviations_importance_max\": max(aggregated[\"deviations_importance_max\"], event[\"deviations_importance_max\"]),\n",
    "        \"deviations_importance_sum\": aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_sum\"],\n",
    "        \"deviations_importance_count\": aggregated[\"deviations_importance_count\"] + 1,\n",
    "        \"deviations_importance_mean\": float(aggregated[\"deviations_importance_sum\"] + event[\"deviations_importance_mean\"]) / (aggregated[\"deviations_importance_count\"] + 1),\n",
    "        \"late_count\": aggregated[\"late_count\"] + event[\"late_count\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e30e9",
   "metadata": {},
   "source": [
    "**Create a Streaming DataFrame and define the aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69c5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streaming DataFrame\n",
    "\n",
    "sdf = app.dataframe(input_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f66ce3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<quixstreams.dataframe.dataframe.StreamingDataFrame at 0x7f92e8968100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast data types\n",
    "sdf[\"scheduled\"] = sdf[\"scheduled\"].apply(datetime.fromisoformat)\n",
    "sdf[\"expected\"] = sdf[\"expected\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert all events to departures feature group\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_fg))\n",
    "\n",
    "# drop expected field since it's not needed in the agg fgs\n",
    "sdf.drop(\"expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39105bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  1st Aggregation - 30 Minutes\n",
    "#\n",
    "\n",
    "# convert datetime to str - for serialization of the state\n",
    "sdf[\"scheduled\"] = sdf[\"scheduled\"].apply(lambda t: t.isoformat())\n",
    "\n",
    "sdf.apply(lambda value: print('Event: ', value))\n",
    "\n",
    "# perform window 1-minute aggregations\n",
    "sdf = (\n",
    "    # Define a tumbling window\n",
    "    sdf.tumbling_window(timedelta(seconds=15))  # set 5 seconds for demo and debugging\n",
    "\n",
    "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
    "    .reduce(reducer=reducer_agg, initializer=initializer_agg)\n",
    "\n",
    "    # Emit results only for closed windows\n",
    "    .final()\n",
    "    \n",
    "    # extract value\n",
    "    .apply(lambda result: result[\"value\"])\n",
    ")\n",
    "\n",
    "# revert timestamp string to datetime\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert to departures_agg feature group\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_agg_30m_fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eba9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  2nd Aggregation - 2 Minutes\n",
    "#\n",
    "\n",
    "# convert datetime to str - for serialization of the state\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(lambda t: t.isoformat())\n",
    "\n",
    "# perform window aggregations for 2 minutes\n",
    "sdf = (\n",
    "    # Define a tumbling window\n",
    "    sdf.tumbling_window(timedelta(seconds=30)) # set 10 seconds for demo and debugging\n",
    "\n",
    "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
    "    .reduce(reducer=reducer_acc_agg, initializer=initializer_acc_agg)\n",
    "\n",
    "    # Emit results only for closed windows\n",
    "    .final()\n",
    "    \n",
    "    # extract value\n",
    "    .apply(lambda result: result[\"value\"])\n",
    ")\n",
    "\n",
    "# revert timestamp string to datetime\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert to departures_agg FG topic\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_agg_1h_fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70d1642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  3rd Aggregation - 3 Minutes\n",
    "#\n",
    "\n",
    "# convert datetime to str - for serialization of the state\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(lambda t: t.isoformat())\n",
    "\n",
    "# perform window aggregations for 2 minutes\n",
    "sdf = (\n",
    "    # Define a tumbling window\n",
    "    sdf.tumbling_window(timedelta(seconds=60))  # set 15 seconds for demo and debugging\n",
    "\n",
    "    # Create a \"reduce\" aggregation with \"reducer\" and \"initializer\" functions\n",
    "    .reduce(reducer=reducer_acc_agg, initializer=initializer_acc_agg)\n",
    "\n",
    "    # Emit results only for closed windows\n",
    "    .final()\n",
    "    \n",
    "    # extract value\n",
    "    .apply(lambda result: result[\"value\"])\n",
    ")\n",
    "\n",
    "# revert timestamp string to datetime\n",
    "sdf[\"first_scheduled\"] = sdf[\"first_scheduled\"].apply(datetime.fromisoformat)\n",
    "\n",
    "# insert to departures_agg FG topic\n",
    "sdf = sdf.update(lambda event: multi_part_insert(event, departures_agg_6h_fg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f9d69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-20 08:20:59,801] [INFO] [quixstreams] : Starting the Application with the config: broker_address=\"{'bootstrap.servers': 'kafka-cluster-kafka-0.kafka-cluster-kafka-brokers.hopsworks.svc:9092'}\" consumer_group=\"my-group-id\" auto_offset_reset=\"earliest\" commit_interval=5.0s commit_every=0 processing_guarantee=\"at-least-once\"\n",
      "[2024-09-20 08:20:59,802] [INFO] [quixstreams] : Topics required for this application: \"departures\"\n",
      "[2024-09-20 08:20:59,803] [INFO] [quixstreams] : Validating Kafka topics exist and are configured correctly...\n",
      "[2024-09-20 08:21:00,101] [INFO] [quixstreams] : Kafka topics validation complete\n",
      "[2024-09-20 08:21:00,102] [INFO] [quixstreams] : Initializing state directory at \"/hopsfs/Jupyter/commute/state/my-group-id\"\n",
      "[2024-09-20 08:21:00,156] [INFO] [quixstreams] : Waiting for incoming messages\n",
      "[2024-09-20 08:21:03,967] [WARNING] [quixstreams] : Failed to open rocksdb partition, cannot acquire a lock. Retrying in 3.0sec.\n",
      "[2024-09-20 08:21:08,010] [WARNING] [quixstreams] : Failed to open rocksdb partition, cannot acquire a lock. Retrying in 3.0sec.\n",
      "[2024-09-20 08:21:12,029] [WARNING] [quixstreams] : Failed to open rocksdb partition, cannot acquire a lock. Retrying in 3.0sec.\n",
      "[2024-09-20 08:21:14,160] [WARNING] [quixstreams] : Application is stopping due to failure, latest checkpoint will not be committed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/partition.py\u001b[0m in \u001b[0;36m_init_rocksdb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_rocksdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 logger.debug(\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/partition.py\u001b[0m in \u001b[0;36m_open_rocksdict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_missing_column_families\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         rdict = Rdict(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: IO error: While fsync: a directory: Input/output error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-50bd82066411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start streaming feature pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataframe)\u001b[0m\n\u001b[1;32m    791\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_recovery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe_composed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processing_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processing_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_ready_partitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/app.py\u001b[0m in \u001b[0;36m_process_message\u001b[0;34m(self, dataframe_composed)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;31m# Serve producer callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_producer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproducer_poll_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsumer_poll_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/rowconsumer.py\u001b[0m in \u001b[0;36mpoll_row\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \"\"\"\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPartitionAssignmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Always propagate errors happened during assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/kafka/consumer.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mraises\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0mclosed\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     def subscribe(\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/kafka/consumer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPartitionAssignmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error during partition assignment\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/kafka/consumer.py\u001b[0m in \u001b[0;36m_on_assign_wrapper\u001b[0;34m(consumer, partitions)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon_assign\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mon_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0m_wrap_assignment_errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/app.py\u001b[0m in \u001b[0;36m_on_assign\u001b[0;34m(self, _, topic_partitions)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mtp_committed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# Assign store partitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 store_partitions = self._state_manager.on_partition_assign(\n\u001b[0m\u001b[1;32m    890\u001b[0m                     \u001b[0mtopic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                     \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/manager.py\u001b[0m in \u001b[0;36mon_partition_assign\u001b[0;34m(self, topic, partition, committed_offset)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mstore_partitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mstore_partition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mstore_partitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recovery_manager\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstore_partitions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/windowed/store.py\u001b[0m in \u001b[0;36massign_partition\u001b[0;34m(self, partition)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWindowedRocksDBStorePartition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         return cast(\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mWindowedRocksDBStorePartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/store.py\u001b[0m in \u001b[0;36massign_partition\u001b[0;34m(self, partition)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partitions_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         store_partition = self.create_new_partition(\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             (\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/windowed/store.py\u001b[0m in \u001b[0;36mcreate_new_partition\u001b[0;34m(self, path, changelog_producer)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchangelog_producer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChangelogProducer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     ) -> WindowedRocksDBStorePartition:\n\u001b[0;32m---> 45\u001b[0;31m         db_partition = WindowedRocksDBStorePartition(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchangelog_producer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchangelog_producer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/windowed/partition.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, options, changelog_producer)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mchangelog_producer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChangelogProducer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     ):\n\u001b[0;32m---> 42\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchangelog_producer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchangelog_producer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/partition.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, options, changelog_producer)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_max_retries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_max_retries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_retry_backoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_retry_backoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_rocksdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cf_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cf_handle_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumnFamily\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/hopsworks_environment/lib/python3.10/site-packages/quixstreams/state/rocksdb/partition.py\u001b[0m in \u001b[0;36m_init_rocksdb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0mattempt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_retry_backoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start streaming feature pipeline\n",
    "\n",
    "app.run(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9baf8f",
   "metadata": {},
   "source": [
    "#### Materialize departures and departures aggregated features into the Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start materialization job\n",
    "departures_fg.materialization_job.run(await_termination=True)\n",
    "departures_agg_30m_fg.materialization_job.run(await_termination=True)\n",
    "departures_agg_1h_fg.materialization_job.run(await_termination=True)\n",
    "departures_agg_6h_fg.materialization_job.run(await_termination=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# departures_agg_30m_fg.materialization_job.run(await_termination=False)\n",
    "# departures_agg_1h_fg.materialization_job.run(await_termination=False)\n",
    "# departures_agg_6h_fg.materialization_job.run(await_termination=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
